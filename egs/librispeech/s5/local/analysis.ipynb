{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from itertools import product\n",
    "from scipy.optimize import curve_fit\n",
    "from analysis_utils import *\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pio.renderers.default = \"vscode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tables\n",
    "\n",
    "print(\"text_df contents\")\n",
    "text_df = read_text_as_df()\n",
    "display(text_df.head())\n",
    "\n",
    "print(\"perp_df contents\")\n",
    "perp_df = read_perps_as_df()\n",
    "perp_df = perp_df.merge(text_df[['utt', 'len']], on='utt')\n",
    "display(perp_df.head())\n",
    "\n",
    "\n",
    "print(\"wer_df contents\")\n",
    "wer_df = read_best_wers_as_df()\n",
    "display(wer_df.head())\n",
    "\n",
    "print(\"uttwer_df contents\")\n",
    "uttwer_df = read_best_uttwers_as_df()\n",
    "uttwer_df = uttwer_df.merge(text_df[['utt', 'len']], on='utt')\n",
    "display(uttwer_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entropy/perplexity by partition and LM\")\n",
    "df = agg_mean_by_lens(perp_df, 'len', 'ent', ['part', 'perplm'])\n",
    "df['perp'] = np.exp(df['ent'])\n",
    "df = df.pivot(values=['ent', 'perp'], index='part', columns='perplm')\n",
    "display(df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('distribution of per-utt entropy by partition and LM')\n",
    "fig = px.violin(\n",
    "    perp_df, x='ent', y='part', color='perplm',\n",
    "    box=True,\n",
    "    labels=dict(ent='Entropy (nats)', part='Partition', lm=\"LM\"),\n",
    "    width=600, height=800,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test of normality of entropy given LM')\n",
    "display(pg.normality(perp_df, dv='ent', group='perplm', method='normaltest').round(3))\n",
    "\n",
    "print(\"pairwise spearman correlations of entropy across LMs\")\n",
    "df = perp_df.pivot(values='ent', index='utt', columns='perplm')\n",
    "display(pg.pairwise_corr(df, columns=df.columns, alternative='greater', method='spearman').round(3))\n",
    "\n",
    "print(\"scatter plot matrix of per-utterance entropy of each LM\")\n",
    "fig = px.scatter_matrix(df, dimensions=df.columns, opacity=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"per-utterance perplexity vs. rank by LM\")\n",
    "\n",
    "df = perp_df.copy()\n",
    "df['rank'] = df.groupby(['perplm'])['perp'].rank()\n",
    "\n",
    "fig = px.scatter(df, x='rank', y='perp', color='perplm', log_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 'dev-clean'\n",
    "latlm = reslm = 'tgsmall'\n",
    "mdl = 'tdnn_1d_sp'\n",
    "desc = f\"({part} partition, {mdl} model, {latlm} lattice LM, and {reslm} rescoring lm)\"\n",
    "\n",
    "df = uttwer_df.loc[\n",
    "    (~uttwer_df['snr'].isnull()) &\n",
    "    (uttwer_df['latlm'] == latlm) &\n",
    "    (uttwer_df['reslm'] == reslm) &\n",
    "    (uttwer_df['part'] == part) &\n",
    "    (uttwer_df['mdl'] == mdl)\n",
    "].copy()\n",
    "df['snr'] = df['snr'].astype('int')\n",
    "\n",
    "with pd.option_context('display.max_rows', 10):\n",
    "    print(f\"test of normality of per-utterance WERs given SNR {desc}\")\n",
    "    display(pg.normality(df, dv='wer', group='snr', method='normaltest').round(3).sort_index())\n",
    "\n",
    "\n",
    "    print(f\"spearman correlation of WERs across SNRs {desc}\")\n",
    "    df = df.pivot(values='wer', index='utt', columns='snr')\n",
    "    display(pg.pairwise_corr(df, columns=df.columns, alternative='greater', method='spearman').round(3).sort_index())\n",
    "\n",
    "print(f\"scatter plot matrix of per-utterance WERs of select SNRs {desc}\")\n",
    "fig = px.scatter_matrix(df, dimensions=[5, 10, 20, 30], opacity=0.1)\n",
    "fig.update_layout({\"xaxis\"+str(i+1): dict(range = [-0.1, 1]) for i in range(len(df.columns))})\n",
    "fig.update_layout({\"yaxis\"+str(i+1): dict(range = [-0.1, 1]) for i in range(len(df.columns))})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang et al (2023) \"Estimate the noise effect on automatic speech recognition\n",
    "# accuracy for mandarin by an approach associating articulation index\"\n",
    "# FIXME(sdrobert): the fit is very bad if we use eq. 12\n",
    "\n",
    "latlm = 'tgsmall'\n",
    "reslm = 'tgsmall'\n",
    "part = 'dev-clean'\n",
    "desc = f\"({part} partition, {latlm} lattice LM, and {reslm} rescoring lm)\"\n",
    "num_points = 100\n",
    "\n",
    "df = wer_df.loc[\n",
    "    (wer_df['latlm'] == latlm) &\n",
    "    (wer_df['reslm'] == reslm) &\n",
    "    (wer_df['part'] == part)\n",
    "].copy()\n",
    "\n",
    "idx = df['snr'].isnull()\n",
    "df, Ainvs = df.loc[~idx], df.loc[idx, ['mdl', 'acc']]\n",
    "snr_min = df['snr'].min() - 1\n",
    "snr_max = df['snr'].max() + 1\n",
    "x_interp = np.linspace(snr_min, snr_max, num_points)\n",
    "\n",
    "mdls = df['mdl'].unique()\n",
    "assert all(mdls == Ainvs['mdl'].unique())\n",
    "ratio = num_points // (len(mdls) + 2)\n",
    "\n",
    "def zhang_func(x : np.ndarray, A : float, B : float, C : float) -> np.ndarray:\n",
    "    return 1 / (np.exp(-(x + B) / C) + A)\n",
    "\n",
    "\n",
    "fit = []\n",
    "fig = go.Figure()\n",
    "for mdl_idx, mdl in enumerate(mdls):\n",
    "    colour = px.colors.qualitative.Plotly[mdl_idx]\n",
    "    df_ = df.loc[df['mdl'] == mdl]\n",
    "    Ainv = Ainvs.loc[Ainvs['mdl'] == mdl, 'acc'].iloc[0]\n",
    "    A_init = 1 / Ainv\n",
    "    N = len(df_)\n",
    "    x = df_['snr'].array\n",
    "    y = df_['acc'].array\n",
    "    (A, B, C), _ = curve_fit(\n",
    "        zhang_func, x, y,\n",
    "        p0=(A_init, 0, 1),\n",
    "        bounds=([1, -np.inf, 0.01], [np.inf, np.inf, np.inf]),\n",
    "    )\n",
    "    y_pred = zhang_func(x, A, B, C)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    fit.append(dict(mdl=mdl, A=A, B=B, C=C, r2=r2))\n",
    "    y_interp = 1 / (A + np.exp(-(x_interp + B) / C))\n",
    "    fig.add_scatter(\n",
    "        x=x, y=df_['acc'] * 100,\n",
    "        name=mdl, mode='markers',\n",
    "        marker=dict(color=colour),\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=x_interp, y=y_interp * 100,\n",
    "        mode='lines',\n",
    "        opacity=0.5,\n",
    "        showlegend=False,\n",
    "        line=dict(color=colour),\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=x_interp[ratio * (mdl_idx + 1)], y=y_interp[ratio * (mdl_idx + 1)] * 100,\n",
    "        text=f\"A={A:.02f},B={B:.02f},C={C:.02f}\",\n",
    "        showarrow=True,\n",
    "        font=dict(color=colour),\n",
    "    )\n",
    "print(f\"Zhang et al fits by model {desc}\")\n",
    "display(pd.DataFrame.from_records(fit).round(3))\n",
    "\n",
    "print(f\"accuracy (inv. WER) by SNR across models w/ Zhang et al fits {desc}\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"SNR (dB)\",\n",
    "    yaxis_title=\"accuracy (%)\",\n",
    "    legend_title=\"model\",\n",
    "    xaxis_tickformat='d',\n",
    "    yaxis_tickformat='d',\n",
    "    xaxis_range=[snr_min, snr_max],\n",
    "    yaxis_range=[0, 100],\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity vs. WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boothroyd's k\n",
    "\n",
    "latlm = reslm = perplm = binlm = 'tgsmall'\n",
    "mdl = 'tdnn_1d_sp'\n",
    "num_bins = 5\n",
    "num_points = 100\n",
    "binpart = 'dev-clean'\n",
    "part = 'dev-clean'\n",
    "x_interp = np.linspace(0.01, 100, num_points)\n",
    "ratio = num_points // (num_bins + 2)\n",
    "add_intercept = False\n",
    "print(\n",
    "    f\"mdl {mdl}, part {part} lattice lm {latlm}, rescore lm {reslm} perplexity LM \"\n",
    "    f\"{perplm}, bin part {binpart}, bin LM {binlm}\"\n",
    ")\n",
    "\n",
    "df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)].copy()\n",
    "bounds = bin_series(perp_df.loc[(perp_df['perplm'] == binlm) & (perp_df['part'] == binpart), 'perp'], num_bins)[1]\n",
    "df['perp_bin'] = bin_series(df['perp'], bounds, by_rank=False, fmt=\"{:.0f}\")[0]\n",
    "bin_cats = df['perp_bin'].dtype.categories\n",
    "\n",
    "print(\"mean entropy by bin and ratio (highest/bin)\")\n",
    "df_ent = agg_mean_by_lens(df, 'len', 'ent', 'perp_bin')\n",
    "df_ent['ratio'] = df_ent.loc[df_ent['perp_bin'] == bin_cats[num_bins - 1], 'ent'].iloc[0] / df_ent['ent']\n",
    "display(df_ent.round(3))\n",
    "\n",
    "df = df.merge(\n",
    "    uttwer_df.loc[\n",
    "        (uttwer_df['reslm'] == reslm) &\n",
    "        (uttwer_df['latlm'] == latlm) &\n",
    "        (uttwer_df['mdl'] == mdl)\n",
    "    ], on=['utt', 'part', 'len'])\n",
    "\n",
    "df = agg_mean_by_lens(df, 'len', 'wer', ['snr', 'perp_bin'])\n",
    "df = df.pivot(values='wer', index='snr', columns='perp_bin')\n",
    "\n",
    "fig_acc, fig_loge = go.Figure(), go.Figure()\n",
    "x = df[bin_cats[num_bins - 1]]\n",
    "\n",
    "fits = []\n",
    "for bin in range(num_bins):\n",
    "    y = df[bin_cats[bin]]\n",
    "    fit : pd.DataFrame = pg.linear_regression(np.log(x), np.log(y), add_intercept=add_intercept)\n",
    "    iv_name, int_name = f\"iv {bin_cats[bin]}\", f\"int {bin_cats[bin]}\"\n",
    "    fit['names'] = fit['names'].map({bin_cats[num_bins - 1]: iv_name, \"Intercept\": int_name})\n",
    "    if add_intercept:\n",
    "        int_ = np.exp(fit.loc[fit['names'] == int_name, 'coef'].iloc[0])\n",
    "    else:\n",
    "        int_ = 1.0\n",
    "    fits.append(fit)\n",
    "    k = fit.loc[fit['names'] == iv_name, 'coef'].iloc[0]\n",
    "    colour = px.colors.qualitative.Plotly[bin]\n",
    "    y_interp = 100 * (1 - int_ * (1 - x_interp / 100) ** k)\n",
    "    interp_name = f\"k={k:.02f}\" + (f\", i={int_:.02f}\" if add_intercept else \"\")\n",
    "    fig_acc.add_scatter(\n",
    "        x=100 - x * 100, y=100 - y * 100,\n",
    "        name=bin_cats[bin], mode='markers',\n",
    "        marker=dict(color=colour),\n",
    "    )\n",
    "    fig_acc.add_scatter(\n",
    "        x=x_interp, y=y_interp,\n",
    "        name=interp_name,\n",
    "        showlegend=False, mode='lines', opacity=0.5,\n",
    "        line=dict(color=colour),\n",
    "    )\n",
    "    fig_acc.add_annotation(\n",
    "        x=x_interp[ratio * (bin + 1)], y=y_interp[ratio * (bin + 1)],\n",
    "        text=interp_name,\n",
    "        showarrow=True,\n",
    "        opacity=1,\n",
    "        font=dict(color=colour),\n",
    "    )\n",
    "    fig_loge.add_scatter(\n",
    "        x=100 * x, y=100 * y,\n",
    "        name=bin_cats[bin], mode='markers',\n",
    "        marker=dict(color=colour),\n",
    "    )\n",
    "    fig_loge.add_scatter(\n",
    "        x=100 - x_interp, y=100 - y_interp,\n",
    "        showlegend=False, mode='lines', opacity=0.5,\n",
    "        name=interp_name,\n",
    "        line=dict(color=colour),\n",
    "    )\n",
    "    fig_loge.add_annotation(\n",
    "        x=np.log10(100 - x_interp[ratio * (bin + 1)]), y=np.log10(100 - y_interp[ratio * (bin + 1)]),\n",
    "        text=interp_name,\n",
    "        showarrow=True,\n",
    "        opacity=1,\n",
    "        font=dict(color=colour),\n",
    "    )\n",
    "print(\"Boothroyd & Nittrouer model fits\")\n",
    "display(pd.concat(fits))\n",
    "\n",
    "print(\"in-context vs. out-of-context accuracy and B & N fits\")\n",
    "fig_acc.update_layout(\n",
    "    xaxis_title=\"out-of-context accuracy (%)\",\n",
    "    yaxis_title=\"in-context accuracy (%)\",\n",
    "    legend_title=\"in-context perp\",\n",
    "    xaxis_tickformat='d',\n",
    "    yaxis_tickformat='d',\n",
    "    xaxis_range=[0, 100],\n",
    "    yaxis_range=[0, 100],\n",
    "    width=800, height=400,\n",
    ")\n",
    "fig_acc.show()\n",
    "print(\"in-context vs. out-of-context error rates and B & N fits\")\n",
    "fig_loge.update_layout(\n",
    "    xaxis_title=\"out-of-context error rate (%)\",\n",
    "    yaxis_title=\"in-context error rate (%)\",\n",
    "    legend_title=\"in-context perp\",\n",
    "    width=800, height=400,\n",
    ")\n",
    "lims = np.log10(df[bin_cats[0]].min() * 100 - 1), np.log10(95)\n",
    "fig_loge.update_xaxes(type='log', range=lims)\n",
    "fig_loge.update_yaxes(type='log', range=lims)\n",
    "fig_loge.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wer by perp\n",
    "\n",
    "mdl = 'tdnn_1d_sp'\n",
    "latlm = reslm = perplm = 'tgsmall'\n",
    "num_points = 100\n",
    "part = 'dev-clean'\n",
    "print(\n",
    "    f\"mdl {mdl}, partition {part}, lattice LM {latlm}, rescore LM {reslm}, \"\n",
    "    f\"perlexity LM {perplm}\"\n",
    ")\n",
    "\n",
    "df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)]\n",
    "df = df.merge(uttwer_df.loc[\n",
    "    (uttwer_df['reslm'] == reslm) &\n",
    "    (uttwer_df['latlm'] == latlm) &\n",
    "    (uttwer_df['mdl'] == mdl)\n",
    "], on=['utt', 'part'])\n",
    "df = df.loc[df['snr'].isnull()]  # without noise\n",
    "ymin, ymax = df['wer'].quantile(0.05), df['wer'].quantile(0.95)\n",
    "xmin, xmax = df['perp'].quantile(0.05), df['perp'].quantile(0.95)\n",
    "perp_interp = np.linspace(xmin, xmax, num_points)\n",
    "\n",
    "print(\"per-utterance WER by perplexity\")\n",
    "fig = px.scatter(df, x='perp', y='wer')\n",
    "fig.update_xaxes(type='log', range=[np.log10(xmin), np.log10(xmax)])\n",
    "fig.update_yaxes(range=[ymin, ymax])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klakow and Peters (2002). \"Testing the correlation of word error rate and perplexity\"\n",
    "# \"... slope a is smaller for tasks that are acoustically more challenging. Hence on\n",
    "# those tasks larger reductions in PP are needed to obtain a given reduction in WER.\" \n",
    "\n",
    "latlm = reslm = perplm = binlm = 'tgsmall'\n",
    "mdl = 'tdnn_1d_sp'\n",
    "num_bins = 5\n",
    "num_points = 100\n",
    "part = binpart = 'dev-clean'\n",
    "print(\n",
    "    f\"mdl {mdl}, part {part} lattice lm {latlm}, rescore lm {reslm} perplexity LM \"\n",
    "    f\"{perplm}, bin part {binpart}, bin LM {binlm}\"\n",
    ")\n",
    "\n",
    "def klakow_func(perp : np.ndarray, a : float, b: float) -> np.ndarray:\n",
    "    return b * (perp ** a)\n",
    "\n",
    "df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)].copy()\n",
    "bounds = bin_series(perp_df.loc[(perp_df['perplm'] == binlm) & (perp_df['part'] == binpart), 'perp'], num_bins)[1]\n",
    "bins = bin_series(df['perp'], bounds, by_rank=False, fmt=\"{:.0f}\")[0]\n",
    "df['perp_bin'] = bins\n",
    "x = agg_mean_by_lens(df, 'len', 'ent', 'perp_bin')['ent']\n",
    "\n",
    "df = df.merge(uttwer_df.loc[\n",
    "    (uttwer_df['reslm'] == reslm) &\n",
    "    (uttwer_df['latlm'] == latlm) &\n",
    "    (uttwer_df['mdl'] == mdl)\n",
    "], on=['utt', 'part', 'len'])\n",
    "df = agg_mean_by_lens(df, 'len', 'wer', ['snr', 'perp_bin', 'mdl'])\n",
    "df = df.reset_index()\n",
    "\n",
    "snrs = df['snr'].unique()\n",
    "snrs.sort()\n",
    "fits = []\n",
    "curve_params_list = []\n",
    "for snr in snrs:\n",
    "    snr_mask = df['snr'] == snr\n",
    "    y = np.log(df.loc[df['snr'] == snr, \"wer\"])\n",
    "    fit : pd.DataFrame = pg.linear_regression(x, y)\n",
    "    curve_params_list.append({\n",
    "        \"snr\": snr,\n",
    "        \"a\": fit.loc[fit['names'] == 'ent', 'coef'].iloc[0],\n",
    "        \"b\": np.exp(fit.loc[fit['names'] == 'Intercept', 'coef'].iloc[0]),\n",
    "    })\n",
    "    iv_name, int_name = f\"iv {int(snr)}\", f\"int {int(snr)}\"\n",
    "    fit['names'] = fit['names'].map({'ent': iv_name, \"Intercept\": int_name})\n",
    "    fits.append(fit)\n",
    "print(\"regression fits for Klakow and Peters models\")\n",
    "display(pd.concat(fits))\n",
    "\n",
    "snr_mini, snr_midi, snr_maxi = 10, 16, len(snrs) - 1\n",
    "df = df.loc[(df['snr'] >= snrs[snr_mini]) & (df['snr'] <= snrs[snr_maxi])]\n",
    "df['wer'] *= 100\n",
    "\n",
    "print(\"WER by (PP, SNR) with select K & P fits\")\n",
    "fig = px.bar(df, x='perp_bin', y='wer', color='snr', barmode='overlay', color_continuous_scale=\"viridis\", opacity=1.0)\n",
    "for dict_ in (curve_params_list[snr_mini], curve_params_list[snr_midi], curve_params_list[snr_maxi]):\n",
    "    y = klakow_func(np.exp(x), dict_['a'], dict_['b']) * 100\n",
    "    interp_name = f\"a={dict_['a']:.03f}, b={dict_['b']:.03f} WER âˆˆ [{y.min():.02f},{y.max():.02f}]\"\n",
    "    fig.add_scatter(\n",
    "        x=bins.dtype.categories,\n",
    "        y=y,\n",
    "        showlegend=False,\n",
    "        name=interp_name,\n",
    "        mode='markers+lines',\n",
    "        marker=dict(color='red'), line=dict(color='red'))\n",
    "    fig.add_annotation(\n",
    "        x=bins.dtype.categories[0], y=y.iloc[0],\n",
    "        text=interp_name,\n",
    "        showarrow=True,\n",
    "        opacity=1,\n",
    "        font=dict(color=\"black\"),\n",
    "        bgcolor='white',\n",
    "    )\n",
    "fig.update_layout(\n",
    "    yaxis_range=[0, 100]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"K & P model parameters by SNR\")\n",
    "df = pd.DataFrame.from_records(curve_params_list)\n",
    "df = pd.melt(df, ['snr'], ['a', 'b'], var_name='param', value_name='val')\n",
    "fig = px.scatter(df, x='snr', y='val', color='param')\n",
    "fig.update_layout(yaxis_range=[0, 1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boothroyd prediction\n",
    "num_bins = 5\n",
    "train_mdl = 'tdnn_1d_sp'\n",
    "train_part = 'dev-clean'\n",
    "train_latlm = train_reslm = train_perplm = 'tgsmall'\n",
    "test_mdls = ('tri6b',)\n",
    "test_parts = ('dev-other',)\n",
    "test_perplms = ('tgmed', 'fglarge')\n",
    "\n",
    "# determine SNRs which don't have extremal values. It is more important to set the\n",
    "# max, as high values tend to inflate correlations (i.e. 0.99^k ~= 0.99)\n",
    "min_wer, max_wer = 0.00, 0.3\n",
    "df = wer_df.loc[\n",
    "    (wer_df['mdl'] == train_mdl) &\n",
    "    (wer_df['latlm'] == train_latlm) &\n",
    "    (wer_df['reslm'] == train_reslm)\n",
    "].groupby('snr')['wer'].agg(['min', 'max'])\n",
    "good_snrs = df.index[(df['min'] >= min_wer) & (df['max'] <= max_wer)]\n",
    "good_snr_min, good_snr_max = good_snrs.min(), good_snrs.max()\n",
    "good_snr_mid = (good_snr_min + good_snr_max) / 2\n",
    "print(f\"good SNRs: [{good_snr_min}, {good_snr_max}]\")\n",
    "\n",
    "# all records we'll consider\n",
    "df = perp_df.copy()\n",
    "bounds = bin_series(\n",
    "    perp_df.loc[\n",
    "        (perp_df['perplm'] == train_perplm) &\n",
    "        (perp_df['part'] == train_part)\n",
    "    , 'ent'], num_bins)[1]\n",
    "df['perp_bin'] = bin_series(df['ent'], bounds, by_rank=False, fmt=\"{:.01f}\")[0]\n",
    "bin_cats = df['perp_bin'].dtype.categories\n",
    "\n",
    "df = df.merge(\n",
    "    uttwer_df.loc[\n",
    "        # ~uttwer_df['snr'].isnull()\n",
    "        (uttwer_df['snr'] >= good_snr_min) & (uttwer_df['snr'] <= good_snr_max)\n",
    "    ], on=['utt', 'part', 'len'])\n",
    "df = agg_mean_by_lens(\n",
    "    df,\n",
    "    'len',\n",
    "    ['wer', 'ent', 'len'],\n",
    "    ['snr', 'perp_bin', 'perplm', 'reslm', 'latlm', 'mdl', 'part'],\n",
    ")\n",
    "df['lwer'] = np.log(df['wer'])\n",
    "\n",
    "train_df = df.loc[\n",
    "    (df['latlm'] == train_latlm) &\n",
    "    (df['reslm'] == train_reslm) &\n",
    "    (df['perplm'] == train_perplm) &\n",
    "    (df['mdl'] == train_mdl) &\n",
    "    (df['part'] == train_part)\n",
    "]\n",
    "\n",
    "print('train entropy by bin')\n",
    "display(train_df.groupby('perp_bin', observed=False)[['ent']].mean().round(3))\n",
    "ent_fit = dict()\n",
    "for in_bin in range(num_bins):\n",
    "    ent_in = train_df.loc[train_df['perp_bin'] == bin_cats[in_bin], 'ent'].iloc[0]\n",
    "    for out_bin in range(num_bins):\n",
    "         ent_out = train_df.loc[train_df['perp_bin'] == bin_cats[out_bin], 'ent'].iloc[0]\n",
    "         ent_fit[(in_bin, out_bin)] = ent_out / ent_in\n",
    "\n",
    "def train(df : pd.DataFrame) -> dict[tuple[int, int], float]:\n",
    "    fits = dict()\n",
    "    for in_bin in range(num_bins):\n",
    "        df_in = df.loc[df['perp_bin'] == bin_cats[in_bin], ['snr', 'lwer']]\n",
    "        for out_bin in range(num_bins):\n",
    "            df_out = df.loc[df['perp_bin'] == bin_cats[out_bin], ['snr', 'lwer']]\n",
    "            df_in_out = df_in.merge(df_out, on='snr', suffixes=('_in', '_out'))\n",
    "            fit = pg.linear_regression(\n",
    "                df_in_out['lwer_out'],\n",
    "                df_in_out['lwer_in'],\n",
    "                add_intercept=False\n",
    "            )\n",
    "            k : float = fit['coef'].iloc[0]\n",
    "            fits[(in_bin, out_bin)] = k\n",
    "    return fits\n",
    "\n",
    "def test(df: pd.DataFrame, fits : dict[(int, int), float], plot : bool = False) -> pd.DataFrame:\n",
    "    res = dict()\n",
    "    for in_bin in range(num_bins):\n",
    "        df_in = df.loc[df['perp_bin'] == bin_cats[in_bin]]\n",
    "        wer_true = ((df_in['wer'] * df_in['len']).sum() / df_in['len'].sum())\n",
    "        df_in = df_in[['snr', 'lwer']]\n",
    "        for out_bin in range(num_bins):\n",
    "            df_out = df.loc[df['perp_bin'] == bin_cats[out_bin]]\n",
    "            k = fits[(in_bin, out_bin)]\n",
    "            wer_pred = ((df_out['wer'] * df_out['len']).sum() / df_out['len'].sum()) ** k\n",
    "            df_out = df_out[['snr', 'lwer']]\n",
    "            df_in_out = df_in.merge(df_out, on='snr', suffixes=('_in', '_out'))\n",
    "            y_true = df_in_out['lwer_in'].to_numpy()\n",
    "            y_pred = k * df_in_out['lwer_out'].to_numpy()\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            res[(in_bin, out_bin)] = r2, 100 * wer_true, 100 * wer_pred\n",
    "    df = pd.DataFrame.from_dict(res, orient='index', columns=['r2', 'wer_true', 'wer_pred'])\n",
    "    df.sort_index()\n",
    "    df.index = pd.MultiIndex.from_product([bin_cats] * 2, names=['in_bin', 'out_bin'])\n",
    "    if plot:\n",
    "        im = df.reset_index().pivot(values='r2', columns='out_bin', index='in_bin')\n",
    "        fig = px.imshow(\n",
    "            im,\n",
    "            labels=dict(x=\"out-of-context bin\", y=\"in-context bin\", z=\"R^2\"),\n",
    "            x=bin_cats,\n",
    "            y=bin_cats,\n",
    "            zmin=-1,\n",
    "            text_auto=\".3f\",\n",
    "            color_continuous_scale='BrBG',\n",
    "        )\n",
    "        fig.show()\n",
    "    return df\n",
    "\n",
    "def display_test(df: pd.DataFrame, groupby=None):\n",
    "    df = df.reset_index()\n",
    "    df = df.reset_index().loc[df['in_bin'] != df['out_bin']].copy()\n",
    "    df['wer_diff'] = np.abs(df['wer_pred'] - df['wer_true'])\n",
    "    if groupby:\n",
    "        df_with = df.groupby(groupby)\n",
    "    else:\n",
    "        df_with = df\n",
    "    df_with = df_with[['r2', 'wer_diff', 'wer_true']].mean()\n",
    "    df = df.loc[\n",
    "        (df['in_bin'] != bin_cats[0]) &\n",
    "        (df['in_bin'] != bin_cats[-1]) &\n",
    "        (df['out_bin'] != bin_cats[0]) &\n",
    "        (df['out_bin'] != bin_cats[-1])\n",
    "    ]\n",
    "    if groupby:\n",
    "        df_wo = df.groupby(groupby)\n",
    "    else:\n",
    "        df_wo = df\n",
    "    df_wo = df_wo[['r2', 'wer_diff', 'wer_true']].mean()\n",
    "    display(pd.concat([df_with, df_wo], keys=['w/ extreme bins', 'w/o extreme bins']).round(3))\n",
    "\n",
    "\n",
    "print('all equal fit on train')\n",
    "display_test(test(\n",
    "    train_df,\n",
    "    dict((key, 1.0) for key in product(range(num_bins), repeat=2))\n",
    "))\n",
    "\n",
    "print('entropy fit on train')\n",
    "display_test(test(train_df, ent_fit, True))\n",
    "\n",
    "print(f'split by SNR {good_snr_mid} and train/test on quadrants')\n",
    "res = dict()\n",
    "for train_split, test_split in product((\"low\", \"high\"), repeat=2):\n",
    "    if train_split == \"low\":\n",
    "        fit = train(train_df.loc[train_df['snr'] <= good_snr_mid])\n",
    "    else:\n",
    "        fit = train(train_df.loc[train_df['snr'] > good_snr_mid])\n",
    "    if test_split == \"low\":\n",
    "        scores = test(train_df.loc[train_df['snr'] <= good_snr_mid], fit)\n",
    "    else:\n",
    "        scores = test(train_df.loc[train_df['snr'] > good_snr_mid], fit)\n",
    "    res[(train_split, test_split)] = scores\n",
    "res = pd.concat(res.values(), keys=res.keys(), names=['train SNR', 'test SNR'])\n",
    "display_test(res, [\"train SNR\", \"test SNR\"])\n",
    "\n",
    "fit = train(train_df)\n",
    "\n",
    "print('train and test on self')\n",
    "display_test(test(train_df, fit))\n",
    "\n",
    "for test_mdl in test_mdls:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == train_perplm) &\n",
    "        (df['mdl'] == test_mdl) &\n",
    "        (df['part'] == train_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_mdl}, test on {test_mdl}\")\n",
    "    display_test(test(test_df, fit))\n",
    "\n",
    "    print(f\"entropy fit on {test_mdl}\")\n",
    "    display_test(test(test_df, ent_fit))\n",
    "\n",
    "\n",
    "for test_part in test_parts:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == train_perplm) &\n",
    "        (df['mdl'] == train_mdl) &\n",
    "        (df['part'] == test_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_part}, test on {test_part}\")\n",
    "    display_test(test(test_df, fit))\n",
    "\n",
    "    print(f\"entropy fit on {test_part}\")\n",
    "    display_test(test(test_df, ent_fit))\n",
    "\n",
    "for test_perplm in test_perplms:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == test_perplm) &\n",
    "        (df['mdl'] == train_mdl) &\n",
    "        (df['part'] == train_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_part}, test on {test_perplm}\")\n",
    "    display_test(test(test_df, fit, True))\n",
    "\n",
    "    print(f\"entropy fit on {test_perplm}\")\n",
    "    display_test(test(test_df, ent_fit, True))\n",
    "\n",
    "# train_fits = dict()\n",
    "# train_bounds = dict()\n",
    "# test_fits = dict()\n",
    "# for train in (True, False):\n",
    "#     for train_key in product(mdls, parts, perplms, latlms, reslms):\n",
    "#         if train:\n",
    "#             keys = [train_key]\n",
    "#         else:\n",
    "#             keys = product(mdls, parts, perplms, latlms, reslms)\n",
    "#             bounds = train_bounds[train_key]\n",
    "#         for key in keys:\n",
    "#             mdl, part, perplm, latlm, reslm = key\n",
    "#             df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)].copy()\n",
    "#             if train:\n",
    "#                 bins, train_bounds[train_key] = bin_series(df['perp'], num_bins)\n",
    "#             else:\n",
    "#                 if key == train_key:\n",
    "#                     continue\n",
    "#                 bins = bin_series(df['perp'], bounds, by_rank=False)[0]\n",
    "#             bin_cats = bins.dtype.categories\n",
    "#             df['perp_bin'] = bins\n",
    "#             df = df[['utt', 'perp_bin']].merge(\n",
    "#                 uttwer_df.loc[\n",
    "#                     (uttwer_df['reslm'] == reslm) &\n",
    "#                     (uttwer_df['latlm'] == latlm) &\n",
    "#                     (uttwer_df['mdl'] == mdl) &\n",
    "#                     (~uttwer_df['snr'].isnull())\n",
    "#                 , ['utt', 'snr', 'wer']], on='utt'\n",
    "#             )\n",
    "#             df = agg_mean_by_lens(df, text_df['len'], 'wer', ['snr', 'perp_bin'])\n",
    "#             df['lwer'] = np.log(df['wer'])\n",
    "#             df = df.drop('wer', axis=1)\n",
    "#             for in_bin in range(num_bins - 1):\n",
    "#                 df_in = df.loc[df['perp_bin'] == bin_cats[in_bin], ['snr', 'lwer']]\n",
    "#                 for out_bin in range(in_bin + 1, num_bins):\n",
    "#                     df_out = df.loc[df['perp_bin'] == bin_cats[out_bin], ['snr', 'lwer']]\n",
    "#                     df_in_out = df_in.merge(df_out, on='snr', suffixes=('_in', '_out'))\n",
    "#                     if train:\n",
    "#                         fit = pg.linear_regression(df_in_out['lwer_out'], df_in_out['lwer_in'], add_intercept=False)\n",
    "#                         k, r2 = fit['coef'].iloc[0], fit['r2'].iloc[0]\n",
    "#                         train_fits[(*train_key, in_bin, out_bin)] = (k, r2)\n",
    "#                     else:\n",
    "#                         k, train_r2 = train_fits[(*train_key, in_bin, out_bin)]\n",
    "#                         y_true = df_in_out['lwer_in'].to_numpy()\n",
    "#                         y_pred = k * df_in_out['lwer_out'].to_numpy()\n",
    "#                         # FIXME(sdrobert): adjust?\n",
    "#                         r2_test = r2_score(y_true, y_pred)\n",
    "#                         test_fits[(*train_key, in_bin, out_bin, *key)] = r2_test\n",
    "\n",
    "# train_cols = (\"train_mdl\", \"train_part\", \"train_perplm\", \"train_latlm\", \"train_reslm\", \"in_bin\", \"out_bin\")\n",
    "# test_cols = train_cols + tuple(x.replace(\"train\", \"test\") for x in train_cols[:-2])\n",
    "# train_fits = pd.DataFrame.from_dict(train_fits, orient='index', columns=['k', 'train_r2'])\n",
    "# train_fits.index = pd.MultiIndex.from_tuples(train_fits.index, names=train_cols)\n",
    "# test_fits = pd.DataFrame.from_dict(test_fits, orient='index', columns=['test_r2'])\n",
    "# test_fits.index = pd.MultiIndex.from_tuples(test_fits.index, names=test_cols)\n",
    "# test_fits.head()\n",
    "# fits = test_fits.join(train_fits, on=train_cols)\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     for train_col in train_cols[:-2]:\n",
    "#         display(test_fits.pivot_table(values='test_r2', columns=[train_col, train_col.replace(\"train\", \"test\")], aggfunc=['min', 'mean', 'median']).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klakow prediction\n",
    "num_bins = 3\n",
    "mdls = ('tdnn_1d_sp', 'tri6b')\n",
    "parts = ('dev-clean', 'dev-other')\n",
    "perplms = ('tgsmall', 'fglarge')\n",
    "latlms = ('tgsmall',)\n",
    "reslms = ('tgsmall',)\n",
    "\n",
    "train_fits = dict()\n",
    "train_bounds = dict()\n",
    "test_fits = dict()\n",
    "for train in (True, False):\n",
    "    for train_key in product(mdls, parts, perplms, latlms, reslms):\n",
    "        if train:\n",
    "            keys = [train_key]\n",
    "        else:\n",
    "            keys = product(mdls, parts, perplms, latlms, reslms)\n",
    "            bounds = train_bounds[train_key]\n",
    "        for key in keys:\n",
    "            mdl, part, perplm, latlm, reslm = key\n",
    "            df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)].copy()\n",
    "            if train:\n",
    "                bins, train_bounds[train_key] = bin_series(df['perp'], num_bins)\n",
    "            else:\n",
    "                if key == train_key:\n",
    "                    continue\n",
    "                bins = bin_series(df['perp'], bounds, by_rank=False)[0]\n",
    "            bin_cats = bins.dtype.categories\n",
    "            df['perp_bin'] = bins\n",
    "            df = df[['utt', 'perp_bin', 'ent']].merge(\n",
    "                uttwer_df.loc[\n",
    "                    (uttwer_df['reslm'] == reslm) &\n",
    "                    (uttwer_df['latlm'] == latlm) &\n",
    "                    (uttwer_df['mdl'] == mdl) &\n",
    "                    (~uttwer_df['snr'].isnull())\n",
    "                , ['utt', 'snr', 'wer']], on='utt'\n",
    "            )\n",
    "            df = agg_mean_by_lens(df, text_df['len'], ['wer', 'ent'], ['snr', 'perp_bin'])\n",
    "            df['lwer'] = np.log(df['wer'])\n",
    "            df = df.drop('wer', axis=1)\n",
    "            # for test_bin in range(num_bins)\n",
    "            #     if train:\n",
    "\n",
    "            # for in_bin in range(num_bins - 1):\n",
    "            #     df_in = df.loc[df['perp_bin'] == bin_cats[in_bin], ['snr', 'lwer']]\n",
    "            #     for out_bin in range(in_bin + 1, num_bins):\n",
    "            #         df_out = df.loc[df['perp_bin'] == bin_cats[out_bin], ['snr', 'lwer']]\n",
    "            #         df_in_out = df_in.merge(df_out, on='snr', suffixes=('_in', '_out'))\n",
    "            #         if train:\n",
    "            #             fit = pg.linear_regression(df_in_out['lwer_out'], df_in_out['lwer_in'], add_intercept=False)\n",
    "            #             k, r2 = fit['coef'].iloc[0], fit['r2'].iloc[0]\n",
    "            #             train_fits[(*train_key, in_bin, out_bin)] = (k, r2)\n",
    "            #         else:\n",
    "            #             k, train_r2 = train_fits[(*train_key, in_bin, out_bin)]\n",
    "            #             y_true = df_in_out['lwer_in'].to_numpy()\n",
    "            #             y_pred = k * df_in_out['lwer_out'].to_numpy()\n",
    "            #             # FIXME(sdrobert): adjust?\n",
    "            #             r2_test = r2_score(y_true, y_pred)\n",
    "            #             test_fits[(*train_key, in_bin, out_bin, *key)] = r2_test\n",
    "\n",
    "# train_idx_names = (\"train_mdl\", \"train_part\", \"train_perplm\", \"train_latlm\", \"train_reslm\", \"in_bin\", \"out_bin\")\n",
    "# test_idx_names = train_idx_names + tuple(x.replace(\"train\", \"test\") for x in train_idx_names[:-2])\n",
    "# train_fits = pd.DataFrame.from_dict(train_fits, orient='index', columns=['k', 'train_r2'])\n",
    "# train_fits.index = pd.MultiIndex.from_tuples(train_fits.index, names=train_idx_names)\n",
    "# test_fits = pd.DataFrame.from_dict(test_fits, orient='index', columns=['test_r2'])\n",
    "# test_fits.index = pd.MultiIndex.from_tuples(test_fits.index, names=test_idx_names)\n",
    "# test_fits.head()\n",
    "# fits = test_fits.join(train_fits, on=train_idx_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
