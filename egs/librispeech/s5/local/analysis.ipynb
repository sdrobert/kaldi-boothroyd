{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from itertools import product\n",
    "from scipy.optimize import curve_fit\n",
    "from analysis_utils import *\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pio.renderers.default = \"vscode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tables\n",
    "\n",
    "print(\"text_df contents\")\n",
    "text_df = read_text_as_df()\n",
    "display(text_df.head())\n",
    "\n",
    "print(\"perp_df contents\")\n",
    "perp_df = read_perps_as_df()\n",
    "perp_df = perp_df.merge(text_df[['utt', 'len']], on='utt')\n",
    "display(perp_df.head())\n",
    "\n",
    "\n",
    "print(\"wer_df contents\")\n",
    "wer_df = read_best_wers_as_df()\n",
    "display(wer_df.head())\n",
    "\n",
    "print(\"uttwer_df contents\")\n",
    "uttwer_df = read_best_uttwers_as_df()\n",
    "uttwer_df = uttwer_df.merge(text_df[['utt', 'len']], on='utt')\n",
    "display(uttwer_df.head())\n",
    "\n",
    "df = agg_mean_by_lens(uttwer_df, 'len', ['wer', 'acc'], ['mdl', 'latlm', 'reslm', 'part', 'snr'])\n",
    "df = df.merge(wer_df, on=['mdl', 'latlm', 'reslm', 'part', 'snr'])\n",
    "print(f\"max WER diff (%) btw uttwer and wer: {np.abs(df['acc_x'] - df['acc_y']).max():.01%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entropy/perplexity by partition and LM\")\n",
    "df = agg_mean_by_lens(perp_df, 'len', 'ent', ['part', 'perplm'])\n",
    "df['perp'] = np.exp(df['ent'])\n",
    "df = df.pivot(values=['ent', 'perp'], index='part', columns='perplm')\n",
    "display(df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('distribution of per-utt entropy by partition and LM')\n",
    "fig = px.violin(\n",
    "    perp_df, x='ent', y='part', color='perplm',\n",
    "    box=True,\n",
    "    labels=dict(ent='Entropy (nats)', part='Partition', lm=\"LM\"),\n",
    "    width=600, height=800,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test of normality of entropy given LM')\n",
    "display(pg.normality(perp_df, dv='ent', group='perplm', method='normaltest').round(3))\n",
    "\n",
    "print(\"pairwise spearman correlations of entropy across LMs\")\n",
    "df = perp_df.pivot(values='ent', index='utt', columns='perplm')\n",
    "display(pg.pairwise_corr(df, columns=df.columns, alternative='greater', method='spearman').round(3))\n",
    "\n",
    "print(\"scatter plot matrix of per-utterance entropy of each LM\")\n",
    "fig = px.scatter_matrix(df, dimensions=df.columns, opacity=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"per-utterance perplexity vs. rank by LM\")\n",
    "\n",
    "df = perp_df.copy()\n",
    "df['rank'] = df.groupby(['perplm'])['perp'].rank()\n",
    "\n",
    "fig = px.scatter(df, x='rank', y='perp', color='perplm', log_y=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 'dev-clean'\n",
    "latlm = reslm = 'tgsmall'\n",
    "mdl = 'tdnn_1d_sp'\n",
    "desc = f\"({part} partition, {mdl} model, {latlm} lattice LM, and {reslm} rescoring lm)\"\n",
    "\n",
    "df = uttwer_df.loc[\n",
    "    np.isfinite(uttwer_df['snr']) &\n",
    "    (uttwer_df['latlm'] == latlm) &\n",
    "    (uttwer_df['reslm'] == reslm) &\n",
    "    (uttwer_df['part'] == part) &\n",
    "    (uttwer_df['mdl'] == mdl)\n",
    "].copy()\n",
    "df['snr'] = df['snr'].astype('int')\n",
    "\n",
    "with pd.option_context('display.max_rows', 10):\n",
    "    print(f\"test of normality of per-utterance WERs given SNR {desc}\")\n",
    "    display(pg.normality(df, dv='wer', group='snr', method='normaltest').round(3).sort_index())\n",
    "\n",
    "\n",
    "    print(f\"spearman correlation of WERs across SNRs {desc}\")\n",
    "    df = df.pivot(values='wer', index='utt', columns='snr')\n",
    "    display(pg.pairwise_corr(df, columns=df.columns, alternative='greater', method='spearman').round(3).sort_index())\n",
    "\n",
    "print(f\"scatter plot matrix of per-utterance WERs of select SNRs {desc}\")\n",
    "fig = px.scatter_matrix(df, dimensions=[5, 10, 20, 30], opacity=0.1)\n",
    "fig.update_layout({\"xaxis\"+str(i+1): dict(range = [-0.1, 1]) for i in range(len(df.columns))})\n",
    "fig.update_layout({\"yaxis\"+str(i+1): dict(range = [-0.1, 1]) for i in range(len(df.columns))})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zhang et al (2023) \"Estimate the noise effect on automatic speech recognition\n",
    "# accuracy for mandarin by an approach associating articulation index\"\n",
    "# FIXME(sdrobert): the fit is very bad if we use eq. 12\n",
    "\n",
    "latlm = 'tgsmall'\n",
    "reslm = 'tgsmall'\n",
    "part = 'dev-clean'\n",
    "desc = f\"({part} partition, {latlm} lattice LM, and {reslm} rescoring lm)\"\n",
    "num_points = 100\n",
    "\n",
    "df = wer_df.loc[\n",
    "    (wer_df['latlm'] == latlm) &\n",
    "    (wer_df['reslm'] == reslm) &\n",
    "    (wer_df['part'] == part)\n",
    "].copy()\n",
    "\n",
    "idx = np.isinf(df['snr'])\n",
    "df, Ainvs = df.loc[~idx], df.loc[idx, ['mdl', 'acc']]\n",
    "snr_min = df['snr'].min() - 1\n",
    "snr_max = df['snr'].max() + 1\n",
    "x_interp = np.linspace(snr_min, snr_max, num_points)\n",
    "\n",
    "mdls = df['mdl'].unique()\n",
    "assert all(mdls == Ainvs['mdl'].unique())\n",
    "ratio = num_points // (len(mdls) + 2)\n",
    "\n",
    "def zhang_func(x : np.ndarray, A : float, B : float, C : float) -> np.ndarray:\n",
    "    return 1 / (np.exp(-(x + B) / C) + A)\n",
    "\n",
    "\n",
    "fit = []\n",
    "fig = go.Figure()\n",
    "for mdl_idx, mdl in enumerate(mdls):\n",
    "    colour = px.colors.qualitative.Plotly[mdl_idx]\n",
    "    df_ = df.loc[df['mdl'] == mdl]\n",
    "    Ainv = Ainvs.loc[Ainvs['mdl'] == mdl, 'acc'].iloc[0]\n",
    "    A_init = 1 / Ainv\n",
    "    N = len(df_)\n",
    "    x = df_['snr'].array\n",
    "    y = df_['acc'].array\n",
    "    (A, B, C), _ = curve_fit(\n",
    "        zhang_func, x, y,\n",
    "        p0=(A_init, 0, 1),\n",
    "        bounds=([1, -np.inf, 0.01], [np.inf, np.inf, np.inf]),\n",
    "    )\n",
    "    y_pred = zhang_func(x, A, B, C)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    fit.append(dict(mdl=mdl, A=A, B=B, C=C, r2=r2))\n",
    "    y_interp = 1 / (A + np.exp(-(x_interp + B) / C))\n",
    "    fig.add_scatter(\n",
    "        x=x, y=df_['acc'] * 100,\n",
    "        name=mdl, mode='markers',\n",
    "        marker=dict(color=colour),\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=x_interp, y=y_interp * 100,\n",
    "        mode='lines',\n",
    "        opacity=0.5,\n",
    "        showlegend=False,\n",
    "        line=dict(color=colour),\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=x_interp[ratio * (mdl_idx + 1)], y=y_interp[ratio * (mdl_idx + 1)] * 100,\n",
    "        text=f\"A={A:.02f},B={B:.02f},C={C:.02f}\",\n",
    "        showarrow=True,\n",
    "        font=dict(color=colour),\n",
    "    )\n",
    "print(f\"Zhang et al fits by model {desc}\")\n",
    "display(pd.DataFrame.from_records(fit).round(3))\n",
    "\n",
    "print(f\"accuracy (inv. WER) by SNR across models w/ Zhang et al fits {desc}\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"SNR (dB)\",\n",
    "    yaxis_title=\"accuracy (%)\",\n",
    "    legend_title=\"model\",\n",
    "    xaxis_tickformat='d',\n",
    "    yaxis_tickformat='d',\n",
    "    xaxis_range=[snr_min, snr_max],\n",
    "    yaxis_range=[0, 100],\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity vs. WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wer by perp\n",
    "\n",
    "mdl = 'tdnn_1d_sp'\n",
    "latlm = reslm = perplm = 'tgsmall'\n",
    "num_points = 100\n",
    "part = 'dev-clean'\n",
    "print(\n",
    "    f\"mdl {mdl}, partition {part}, lattice LM {latlm}, rescore LM {reslm}, \"\n",
    "    f\"perlexity LM {perplm}\"\n",
    ")\n",
    "\n",
    "df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)]\n",
    "df = df.merge(uttwer_df.loc[\n",
    "    (uttwer_df['reslm'] == reslm) &\n",
    "    (uttwer_df['latlm'] == latlm) &\n",
    "    (uttwer_df['mdl'] == mdl)\n",
    "], on=['utt', 'part'])\n",
    "df = df.loc[df['snr'].isnull()]  # without noise\n",
    "ymin, ymax = df['wer'].quantile(0.05), df['wer'].quantile(0.95)\n",
    "xmin, xmax = df['perp'].quantile(0.05), df['perp'].quantile(0.95)\n",
    "perp_interp = np.linspace(xmin, xmax, num_points)\n",
    "\n",
    "print(\"per-utterance WER by perplexity\")\n",
    "fig = px.scatter(df, x='perp', y='wer')\n",
    "fig.update_xaxes(type='log', range=[np.log10(xmin), np.log10(xmax)])\n",
    "fig.update_yaxes(range=[ymin, ymax])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boothroyd's k\n",
    "\n",
    "latlm = reslm = perplm = binlm = 'tgsmall'\n",
    "mdl = 'tdnn_1d_sp'\n",
    "num_bins = 5\n",
    "num_points = 100\n",
    "binpart = 'dev-clean'\n",
    "part = 'dev-clean'\n",
    "x_interp = np.linspace(0.01, 100, num_points)\n",
    "ratio = num_points // (num_bins + 2)\n",
    "add_intercept = True\n",
    "print(\n",
    "    f\"mdl {mdl}, part {part} lattice lm {latlm}, rescore lm {reslm} perplexity LM \"\n",
    "    f\"{perplm}, bin part {binpart}, bin LM {binlm}\"\n",
    ")\n",
    "\n",
    "df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)].copy()\n",
    "bounds = bin_series(perp_df.loc[(perp_df['perplm'] == binlm) & (perp_df['part'] == binpart), 'ent'], num_bins)[1]\n",
    "df['ent_bin'] = bin_series(df['ent'], bounds, by_rank=False, fmt=\"{:.01f}\")[0]\n",
    "bin_cats = df['ent_bin'].dtype.categories\n",
    "\n",
    "print(\"mean entropy by bin and ratio (highest/bin)\")\n",
    "df_ent = agg_mean_by_lens(df, 'len', 'ent', 'ent_bin')\n",
    "df_ent['ratio'] = df_ent.loc[df_ent['ent_bin'] == bin_cats[num_bins - 1], 'ent'].iloc[0] / df_ent['ent']\n",
    "display(df_ent.round(3))\n",
    "\n",
    "df = df.merge(\n",
    "    uttwer_df.loc[\n",
    "        (uttwer_df['reslm'] == reslm) &\n",
    "        (uttwer_df['latlm'] == latlm) &\n",
    "        (uttwer_df['mdl'] == mdl)\n",
    "    ], on=['utt', 'part', 'len'])\n",
    "\n",
    "df = agg_mean_by_lens(df, 'len', 'wer', ['snr', 'ent_bin'])\n",
    "df = df.pivot(values='wer', index='snr', columns='ent_bin')\n",
    "\n",
    "fig_acc, fig_loge = go.Figure(), go.Figure()\n",
    "x = df[bin_cats[num_bins - 1]]\n",
    "log_x_lims = np.log10(100 * df[bin_cats[num_bins - 1]].min() - 1), np.log10(30)\n",
    "log_y_lims = np.log10(100 * df[bin_cats[0]].min() - 1), np.log10(30)\n",
    "log_x_interp = np.linspace(log_x_lims[0], np.log10(100), num_points)\n",
    "\n",
    "fits = []\n",
    "for bin in range(num_bins):\n",
    "    y = df[bin_cats[bin]]\n",
    "    fit : pd.DataFrame = pg.linear_regression(np.log(x), np.log(y), add_intercept=add_intercept)\n",
    "    iv_name, int_name = f\"k {bin_cats[bin]}\", f\"c {bin_cats[bin]}\"\n",
    "    fit['names'] = fit['names'].map({bin_cats[num_bins - 1]: iv_name, \"Intercept\": int_name})\n",
    "    if add_intercept:\n",
    "        c = fit.loc[fit['names'] == int_name, 'coef'].iloc[0]\n",
    "    else:\n",
    "        c = 0\n",
    "    fits.append(fit)\n",
    "    k = fit.loc[fit['names'] == iv_name, 'coef'].iloc[0]\n",
    "    colour = px.colors.qualitative.Plotly[bin]\n",
    "    y_interp = 100 * (1 - np.exp(c) * (1 - x_interp / 100) ** k)\n",
    "    interp_name = f\"k={k:.02f}\" + (f\", c={c:.02f}\" if add_intercept else \"\")\n",
    "    fig_acc.add_scatter(\n",
    "        x=100 - x * 100, y=100 - y * 100,\n",
    "        name=bin_cats[bin],\n",
    "        mode='markers',\n",
    "        legendgroup=\"points\",\n",
    "        marker=dict(color=colour),\n",
    "    )\n",
    "    fig_acc.add_scatter(\n",
    "        x=x_interp, y=y_interp,\n",
    "        name=interp_name,\n",
    "        legendgroup=\"fits\",\n",
    "        mode='lines', opacity=0.5,\n",
    "        line=dict(color=colour),\n",
    "    )\n",
    "    fig_loge.add_scatter(\n",
    "        x=100 * x, y=100 * y,\n",
    "        name=bin_cats[bin],\n",
    "        mode='markers',\n",
    "        legendgroup=\"points\",\n",
    "        marker=dict(color=colour),\n",
    "    )\n",
    "    y_interp = 10 ** (k * (log_x_interp - np.log10(100)) + c + np.log10(100))\n",
    "    fig_loge.add_scatter(\n",
    "        x=10 ** (log_x_interp), y=y_interp,\n",
    "        mode='lines',\n",
    "        opacity=0.5,\n",
    "        name=interp_name,\n",
    "        legendgroup=\"fits\",\n",
    "        line=dict(color=colour),\n",
    "    )\n",
    "print(\"Boothroyd & Nittrouer model fits\")\n",
    "display(pd.concat(fits).round(3))\n",
    "\n",
    "print(\"in-context vs. out-of-context accuracy and B & N fits\")\n",
    "fig_acc.update_layout(\n",
    "    xaxis_title=\"out-of-context accuracy (%)\",\n",
    "    yaxis_title=\"in-context accuracy (%)\",\n",
    "    xaxis_tickformat='d',\n",
    "    yaxis_tickformat='d',\n",
    "    xaxis_range=[0, 100],\n",
    "    yaxis_range=[0, 100],\n",
    "    width=800, height=400,\n",
    ")\n",
    "fig_acc.show()\n",
    "print(\"in-context vs. out-of-context error rates and B & N fits\")\n",
    "fig_loge.update_layout(\n",
    "    xaxis_title=\"out-of-context error rate (%)\",\n",
    "    yaxis_title=\"in-context error rate (%)\",\n",
    "    width=800, height=400,\n",
    ")\n",
    "fig_loge.update_xaxes(type='log', range=log_x_lims)\n",
    "fig_loge.update_yaxes(type='log', range=log_y_lims)\n",
    "fig_loge.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klakow and Peters (2002). \"Testing the correlation of word error rate and perplexity\"\n",
    "# \"... slope a is smaller for tasks that are acoustically more challenging. Hence on\n",
    "# those tasks larger reductions in PP are needed to obtain a given reduction in WER.\" \n",
    "\n",
    "latlm = reslm = perplm = binlm = 'tgsmall'\n",
    "mdl = 'tdnn_1d_sp'\n",
    "num_bins = 5\n",
    "num_points = 100\n",
    "part = binpart = 'dev-clean'\n",
    "print(\n",
    "    f\"mdl {mdl}, part {part} lattice lm {latlm}, rescore lm {reslm} perplexity LM \"\n",
    "    f\"{perplm}, bin part {binpart}, bin LM {binlm}\"\n",
    ")\n",
    "\n",
    "def klakow_func(perp : np.ndarray, a : float, b: float) -> np.ndarray:\n",
    "    return b * (perp ** a)\n",
    "\n",
    "df = perp_df.loc[(perp_df['perplm'] == perplm) & (perp_df['part'] == part)].copy()\n",
    "bounds = bin_series(perp_df.loc[(perp_df['perplm'] == binlm) & (perp_df['part'] == binpart), 'ent'], num_bins)[1]\n",
    "bins = bin_series(df['ent'], bounds, by_rank=False, fmt=\"{:.01f}\")[0]\n",
    "df['ent_bin'] = bins\n",
    "bin_cats = df['ent_bin'].dtype.categories\n",
    "x = agg_mean_by_lens(df, 'len', 'ent', 'ent_bin')['ent']\n",
    "print(\"entropy by bin\")\n",
    "display(x.round(3))\n",
    "\n",
    "df = df.merge(uttwer_df.loc[\n",
    "    (uttwer_df['reslm'] == reslm) &\n",
    "    (uttwer_df['latlm'] == latlm) &\n",
    "    (uttwer_df['mdl'] == mdl) &\n",
    "    np.isfinite(uttwer_df['snr'])\n",
    "], on=['utt', 'part', 'len'])\n",
    "display(df.head())\n",
    "snr_30 = agg_mean_by_lens(df, 'len', 'wer', ['snr'])\n",
    "snr_30 = snr_30.loc[snr_30['wer'] < .3, 'snr'].min()\n",
    "df = agg_mean_by_lens(df, 'len', 'wer', ['snr', 'ent_bin'])\n",
    "\n",
    "snrs = df['snr'].unique()\n",
    "snrs.sort()\n",
    "fits = []\n",
    "curve_params_list = []\n",
    "for snr in snrs:\n",
    "    snr_mask = df['snr'] == snr\n",
    "    y = np.log(df.loc[df['snr'] == snr, \"wer\"])\n",
    "    fit : pd.DataFrame = pg.linear_regression(x, y)\n",
    "    curve_params_list.append({\n",
    "        \"snr\": snr,\n",
    "        \"a\": fit.loc[fit['names'] == 'ent', 'coef'].iloc[0],\n",
    "        \"b\": np.exp(fit.loc[fit['names'] == 'Intercept', 'coef'].iloc[0]),\n",
    "    })\n",
    "    iv_name, int_name = f\"iv {int(snr)}\", f\"int {int(snr)}\"\n",
    "    fit['names'] = fit['names'].map({'ent': iv_name, \"Intercept\": int_name})\n",
    "    fits.append(fit)\n",
    "print(\"regression fits for Klakow and Peters models\")\n",
    "display(pd.concat(fits).round(3))\n",
    "\n",
    "snr_mini, snr_midi, snr_maxi = 10, 16, len(snrs) - 1\n",
    "df = df.loc[(df['snr'] >= snrs[snr_mini]) & (df['snr'] <= snrs[snr_maxi])]\n",
    "df['wer'] *= 100\n",
    "\n",
    "print(\"WER by (PP, SNR) with select K & P fits\")\n",
    "fig = px.bar(df, x='ent_bin', y='wer', color='snr', barmode='overlay', color_continuous_scale=\"viridis\", opacity=1.0)\n",
    "for dict_ in (curve_params_list[snr_mini], curve_params_list[snr_midi], curve_params_list[snr_maxi]):\n",
    "    y = klakow_func(np.exp(x), dict_['a'], dict_['b']) * 100\n",
    "    interp_name = f\"a={dict_['a']:.03f}, b={dict_['b']:.03f} WER âˆˆ [{y.min():.02f},{y.max():.02f}]\"\n",
    "    fig.add_scatter(\n",
    "        x=bins.dtype.categories,\n",
    "        y=y,\n",
    "        showlegend=False,\n",
    "        name=interp_name,\n",
    "        mode='markers+lines',\n",
    "        marker=dict(color='red'), line=dict(color='red'))\n",
    "    fig.add_annotation(\n",
    "        x=bins.dtype.categories[0], y=y.iloc[0],\n",
    "        text=interp_name,\n",
    "        showarrow=True,\n",
    "        opacity=1,\n",
    "        font=dict(color=\"black\"),\n",
    "        bgcolor='white',\n",
    "    )\n",
    "fig.update_layout(\n",
    "    yaxis_range=[0, 100]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "df = pd.DataFrame.from_records(curve_params_list)\n",
    "df['logb/a'] = np.log(df['b']) / df['a']\n",
    "print('K & P model parameter ratio by snr')\n",
    "fig = px.scatter(df, x='snr', y='logb/a')\n",
    "fig.show()\n",
    "print(\"K & P model parameters by SNR\")\n",
    "df = pd.melt(df, ['snr'], ['a', 'b'], var_name='param', value_name='val')\n",
    "fig = px.scatter(df, x='snr', y='val', color='param')\n",
    "fig.update_layout(yaxis_range=[0, 1])\n",
    "fig.show()\n",
    "\n",
    "print(\"Predicted k by bin and snr\")\n",
    "records = []\n",
    "ent_out = x[num_bins - 1]\n",
    "for snri, dict_ in enumerate(curve_params_list):\n",
    "    a, b = dict_['a'], dict_['b']\n",
    "    snr = int(snrs[snri])\n",
    "    log_b = np.log(b)\n",
    "    lwer_out = a * ent_out + log_b\n",
    "    for bin_in in (0, num_bins // 2, num_bins - 1):\n",
    "        ent_in = x[bin_in]\n",
    "        ratio_name = f'{bin_cats[bin_in]} over {bin_cats[num_bins - 1]}'\n",
    "        lwer_in = a * ent_in + log_b\n",
    "        k = lwer_in / lwer_out\n",
    "        records.append(dict(snr=snr, k=k, ratio_name=ratio_name))\n",
    "df = pd.DataFrame.from_records(records)\n",
    "fig = px.scatter(df, x='snr', y='k', color='ratio_name')\n",
    "fig.add_vline(x=snr_30, line_dash='dash', line_color='black', annotation_text='70% acc')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boothroyd prediction\n",
    "num_bins = 7\n",
    "train_mdl = 'tdnn_1d_sp'\n",
    "train_part = 'dev-clean'\n",
    "train_latlm = train_reslm = train_perplm = 'tgsmall'\n",
    "test_mdls = ('tri6b',)\n",
    "test_parts = ('dev-other',)\n",
    "test_perplms = ('tgmed', 'fglarge')\n",
    "add_intercept = False\n",
    "\n",
    "# determine SNRs which don't have extremal values. It is more important to set the\n",
    "# max, as high values tend to inflate correlations (i.e. 0.99^k ~= 0.99)\n",
    "min_wer, max_wer = 0.0, 0.20\n",
    "df = wer_df.loc[\n",
    "    (wer_df['mdl'] == train_mdl) &\n",
    "    (wer_df['latlm'] == train_latlm) &\n",
    "    (wer_df['reslm'] == train_reslm) &\n",
    "    (wer_df['part'] == train_part) &\n",
    "    np.isfinite(wer_df['snr'])\n",
    "].groupby('snr')['wer'].agg(['min', 'max'])\n",
    "good_snrs = df.index[(df['min'] >= min_wer) & (df['max'] <= max_wer)]\n",
    "good_snr_min, good_snr_max = good_snrs.min(), good_snrs.max()\n",
    "good_snr_mid = (good_snr_min + good_snr_max) / 2\n",
    "print(f\"good SNRs: [{good_snr_min}, {good_snr_max}]\")\n",
    "\n",
    "# all records we'll consider\n",
    "df = perp_df.copy()\n",
    "bounds = bin_series(\n",
    "    perp_df.loc[\n",
    "        (perp_df['perplm'] == train_perplm) &\n",
    "        (perp_df['part'] == train_part)\n",
    "    , 'ent'], num_bins)[1]\n",
    "df['perp_bin'] = bin_series(df['ent'], bounds, by_rank=False, fmt=\"{:.01f}\")[0]\n",
    "bin_cats = df['perp_bin'].dtype.categories\n",
    "\n",
    "df = df.merge(\n",
    "    uttwer_df.loc[\n",
    "        np.isinf(uttwer_df['snr']) |\n",
    "        ((uttwer_df['snr'] >= good_snr_min) & (uttwer_df['snr'] <= good_snr_max))\n",
    "    ], on=['utt', 'part', 'len'])\n",
    "df = agg_mean_by_lens(\n",
    "    df,\n",
    "    'len',\n",
    "    ['wer', 'ent', 'len'],\n",
    "    ['snr', 'perp_bin', 'perplm', 'reslm', 'latlm', 'mdl', 'part'],\n",
    ")\n",
    "df['lwer'] = np.log(df['wer'])\n",
    "\n",
    "train_df = df.loc[\n",
    "    (df['latlm'] == train_latlm) &\n",
    "    (df['reslm'] == train_reslm) &\n",
    "    (df['perplm'] == train_perplm) &\n",
    "    (df['mdl'] == train_mdl) &\n",
    "    (df['part'] == train_part)\n",
    "]\n",
    "\n",
    "print('train entropy by bin')\n",
    "display(train_df.groupby('perp_bin', observed=False)[['ent']].mean().round(3))\n",
    "ent_fit = dict()\n",
    "for in_bin in range(num_bins):\n",
    "    ent_in = train_df.loc[train_df['perp_bin'] == bin_cats[in_bin], 'ent'].iloc[0]\n",
    "    for out_bin in range(num_bins):\n",
    "         ent_out = train_df.loc[train_df['perp_bin'] == bin_cats[out_bin], 'ent'].iloc[0]\n",
    "         ent_fit[(in_bin, out_bin)] = ent_out / ent_in, 0\n",
    "\n",
    "def train(df : pd.DataFrame) -> dict[tuple[int, int], tuple[float,float]]:\n",
    "    fits = dict()\n",
    "    df = df.loc[np.isfinite(df['snr'])]\n",
    "    for in_bin in range(num_bins):\n",
    "        df_in = df.loc[df['perp_bin'] == bin_cats[in_bin], ['snr', 'lwer']]\n",
    "        for out_bin in range(num_bins):\n",
    "            df_out = df.loc[df['perp_bin'] == bin_cats[out_bin], ['snr', 'lwer']]\n",
    "            df_in_out = df_in.merge(df_out, on='snr', suffixes=('_in', '_out'))\n",
    "            fit = pg.linear_regression(\n",
    "                df_in_out['lwer_out'],\n",
    "                df_in_out['lwer_in'],\n",
    "                add_intercept=add_intercept,\n",
    "            )\n",
    "            k = fit.loc[fit['names'] == 'lwer_out', 'coef'].iloc[0]\n",
    "            if add_intercept:\n",
    "                c = fit.loc[fit['names'] == 'Intercept', 'coef'].iloc[0]\n",
    "            else:\n",
    "                c = 0\n",
    "            fits[(in_bin, out_bin)] = k, c\n",
    "    return fits\n",
    "\n",
    "def test(df: pd.DataFrame, fits : dict[(int, int), tuple[float, float]], plot : bool = False) -> pd.DataFrame:\n",
    "    res = dict()\n",
    "    is_inf = np.isinf(df['snr'])\n",
    "    df_nonoise, df = df.loc[is_inf], df.loc[~is_inf]\n",
    "    for in_bin in range(num_bins):\n",
    "        df_in = df.loc[df['perp_bin'] == bin_cats[in_bin]]\n",
    "        wer_true = df_nonoise.loc[df_nonoise['perp_bin'] == bin_cats[in_bin], 'wer'].iloc[0]\n",
    "        df_in = df_in[['snr', 'lwer']]\n",
    "        for out_bin in range(num_bins):\n",
    "            df_out = df.loc[df['perp_bin'] == bin_cats[out_bin]]\n",
    "            k, c = fits[(in_bin, out_bin)]\n",
    "            wer_pred = df_nonoise.loc[df_nonoise['perp_bin'] == bin_cats[out_bin], 'wer'].iloc[0] ** k\n",
    "            df_out = df_out[['snr', 'lwer']]\n",
    "            df_in_out = df_in.merge(df_out, on='snr', suffixes=('_in', '_out'))\n",
    "            y_true = df_in_out['lwer_in'].to_numpy()\n",
    "            y_pred = k * df_in_out['lwer_out'].to_numpy() + c\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            res[(in_bin, out_bin)] = r2, 100 * wer_true, 100 * wer_pred\n",
    "    df = pd.DataFrame.from_dict(res, orient='index', columns=['r2', 'wer_true', 'wer_pred'])\n",
    "    df.sort_index()\n",
    "    df.index = pd.MultiIndex.from_product([bin_cats] * 2, names=['in_bin', 'out_bin'])\n",
    "    if plot:\n",
    "        im = df.reset_index().pivot(values='r2', columns='out_bin', index='in_bin')\n",
    "        fig = px.imshow(\n",
    "            im,\n",
    "            labels=dict(x=\"out-of-context bin\", y=\"in-context bin\", z=\"R^2\"),\n",
    "            x=bin_cats,\n",
    "            y=bin_cats,\n",
    "            zmin=-1,\n",
    "            text_auto=\".3f\",\n",
    "            color_continuous_scale='BrBG',\n",
    "        )\n",
    "        fig.show()\n",
    "    return df\n",
    "\n",
    "def display_test(df: pd.DataFrame, groupby=None):\n",
    "    df = df.reset_index()\n",
    "    df = df.reset_index().loc[df['in_bin'] != df['out_bin']].copy()\n",
    "    df['wer_diff'] = np.abs(df['wer_pred'] - df['wer_true'])\n",
    "    df['wer_prop'] = df['wer_diff'] / df['wer_true'] * 100\n",
    "    if groupby:\n",
    "        df_with = df.groupby(groupby)\n",
    "    else:\n",
    "        df_with = df\n",
    "    df_with = df_with[['r2', 'wer_diff', 'wer_true', 'wer_prop']].describe()\n",
    "    df = df.loc[\n",
    "        (df['in_bin'] != bin_cats[0]) &\n",
    "        (df['in_bin'] != bin_cats[-1]) &\n",
    "        (df['out_bin'] != bin_cats[0]) &\n",
    "        (df['out_bin'] != bin_cats[-1])\n",
    "    ]\n",
    "    if groupby:\n",
    "        df_wo = df.groupby(groupby)\n",
    "    else:\n",
    "        df_wo = df\n",
    "    df_wo = df_wo[['r2', 'wer_diff', 'wer_true', 'wer_prop']].describe()\n",
    "    df = pd.concat([df_with, df_wo], keys=['w/ extreme bins', 'w/o extreme bins'])\n",
    "    display(df.transpose().round(3))\n",
    "\n",
    "\n",
    "print('all equal fit on train')\n",
    "display_test(test(\n",
    "    train_df,\n",
    "    dict((key, (1, 0)) for key in product(range(num_bins), repeat=2))\n",
    "))\n",
    "\n",
    "print('entropy fit on train')\n",
    "display_test(test(train_df, ent_fit, True))\n",
    "\n",
    "# print(f'split by SNR {good_snr_mid} and train/test on quadrants')\n",
    "# res = dict()\n",
    "# for train_split, test_split in product((\"low\", \"high\"), repeat=2):\n",
    "#     if train_split == \"low\":\n",
    "#         fit = train(train_df.loc[train_df['snr'] <= good_snr_mid])\n",
    "#     else:\n",
    "#         fit = train(train_df.loc[train_df['snr'] > good_snr_mid])\n",
    "#     if test_split == \"low\":\n",
    "#         scores = test(train_df.loc[train_df['snr'] <= good_snr_mid], fit)\n",
    "#     else:\n",
    "#         scores = test(train_df.loc[train_df['snr'] > good_snr_mid], fit)\n",
    "#     res[(train_split, test_split)] = scores\n",
    "# res = pd.concat(res.values(), keys=res.keys(), names=['train SNR', 'test SNR'])\n",
    "# display_test(res, [\"train SNR\", \"test SNR\"])\n",
    "\n",
    "fit = train(train_df)\n",
    "\n",
    "print('train and test on self')\n",
    "display_test(test(train_df, fit))\n",
    "\n",
    "for test_mdl in test_mdls:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == train_perplm) &\n",
    "        (df['mdl'] == test_mdl) &\n",
    "        (df['part'] == train_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_mdl}, test on {test_mdl}\")\n",
    "    display_test(test(test_df, fit))\n",
    "\n",
    "    print(f\"entropy fit on {test_mdl}\")\n",
    "    display_test(test(test_df, ent_fit))\n",
    "\n",
    "\n",
    "for test_part in test_parts:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == train_perplm) &\n",
    "        (df['mdl'] == train_mdl) &\n",
    "        (df['part'] == test_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_part}, test on {test_part}\")\n",
    "    display_test(test(test_df, fit))\n",
    "\n",
    "    print(f\"entropy fit on {test_part}\")\n",
    "    display_test(test(test_df, ent_fit))\n",
    "\n",
    "for test_perplm in test_perplms:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == test_perplm) &\n",
    "        (df['mdl'] == train_mdl) &\n",
    "        (df['part'] == train_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_part}, test on {test_perplm}\")\n",
    "    display_test(test(test_df, fit, True))\n",
    "\n",
    "    print(f\"entropy fit on {test_perplm}\")\n",
    "    display_test(test(test_df, ent_fit, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klakow prediction\n",
    "num_bins = 7\n",
    "train_mdl = 'tdnn_1d_sp'\n",
    "train_part = 'dev-clean'\n",
    "train_latlm = train_reslm = train_perplm = 'tgsmall'\n",
    "test_mdls = ('tri6b',)\n",
    "test_parts = ('dev-other',)\n",
    "test_perplms = ('tgmed', 'fglarge')\n",
    "\n",
    "df = perp_df.copy()\n",
    "bounds = bin_series(\n",
    "    perp_df.loc[\n",
    "        (perp_df['perplm'] == train_perplm) &\n",
    "        (perp_df['part'] == train_part)\n",
    "    , 'ent'], num_bins)[1]\n",
    "df['perp_bin'] = bin_series(df['ent'], bounds, by_rank=False, fmt=\"{:.01f}\")[0]\n",
    "bin_cats = df['perp_bin'].dtype.categories\n",
    "\n",
    "# Klakow's model doesn't have anything to do with SNR\n",
    "df = df.merge(\n",
    "    uttwer_df.loc[\n",
    "        np.isinf(uttwer_df['snr'])\n",
    "    ], on=['utt', 'part', 'len'])\n",
    "df = agg_mean_by_lens(\n",
    "    df,\n",
    "    'len',\n",
    "    ['wer', 'ent', 'len'],\n",
    "    ['perp_bin', 'perplm', 'reslm', 'latlm', 'mdl', 'part'],\n",
    ")\n",
    "df['lwer'] = np.log(df['wer'])\n",
    "\n",
    "train_df = df.loc[\n",
    "    (df['latlm'] == train_latlm) &\n",
    "    (df['reslm'] == train_reslm) &\n",
    "    (df['perplm'] == train_perplm) &\n",
    "    (df['mdl'] == train_mdl) &\n",
    "    (df['part'] == train_part)\n",
    "]\n",
    "\n",
    "def train(df: pd.DataFrame) -> tuple[float, float]:\n",
    "    fit = pg.linear_regression(df['ent'], df['lwer'])\n",
    "    a = fit.loc[fit['names'] == 'ent', 'coef'].iloc[0]\n",
    "    log_b = fit.loc[fit['names'] == 'Intercept', 'coef'].iloc[0]\n",
    "    return a, log_b\n",
    "\n",
    "def test(df: pd.DataFrame, fit: tuple[float, float]) -> dict[str, float]:\n",
    "    wer_true = (df['wer'] * df['len']).sum() / df['len'].sum() * 100\n",
    "    ent = (df['ent'] * df['len']).sum() / df['len'].sum()\n",
    "    wer_pred = np.exp(fit[0] * ent + fit[1]) * 100\n",
    "    y_true = df['lwer'].to_numpy()\n",
    "    y_pred = fit[0] * df['ent'].to_numpy() + fit[1]\n",
    "    if len(y_pred) > 1:\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "    else:\n",
    "        r2 = None\n",
    "    return dict(r2=r2, wer_true=wer_true, wer_pred=wer_pred)\n",
    "\n",
    "def display_test(records : list[dict], groupby=None):\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df['wer_diff'] = np.abs(df['wer_pred'] - df['wer_true'])\n",
    "    df['wer_prop'] = df['wer_diff'] / df['wer_true'] * 100\n",
    "    if groupby:\n",
    "        df = df.groupby(groupby)\n",
    "    df = df[['r2', 'wer_diff', 'wer_true', 'wer_prop']]\n",
    "    if len(records) > 1:\n",
    "        df = df.describe()\n",
    "        df = df.transpose()\n",
    "    display(df.round(3))\n",
    "\n",
    "print(f\"{num_bins}-fold cross-validation\")\n",
    "records = []\n",
    "for test_bin in range(num_bins):\n",
    "    test_mask = train_df['perp_bin'] == bin_cats[test_bin]\n",
    "    records.append(test(train_df[test_mask], train(train_df.loc[~test_mask])))\n",
    "display_test(records)\n",
    "\n",
    "print(\"train and test on self\")\n",
    "fit = train(train_df)\n",
    "display_test([test(train_df, fit)])\n",
    "\n",
    "for test_mdl in test_mdls:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == train_perplm) &\n",
    "        (df['mdl'] == test_mdl) &\n",
    "        (df['part'] == train_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_mdl}, test on {test_mdl}\")\n",
    "    display_test([test(test_df, fit)])\n",
    "\n",
    "\n",
    "for test_part in test_parts:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == train_perplm) &\n",
    "        (df['mdl'] == train_mdl) &\n",
    "        (df['part'] == test_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_part}, test on {test_part}\")\n",
    "    display_test([test(test_df, fit)])\n",
    "\n",
    "for test_perplm in test_perplms:\n",
    "    test_df = df.loc[\n",
    "        (df['latlm'] == train_latlm) &\n",
    "        (df['reslm'] == train_reslm) &\n",
    "        (df['perplm'] == test_perplm) &\n",
    "        (df['mdl'] == train_mdl) &\n",
    "        (df['part'] == train_part)\n",
    "    ]\n",
    "\n",
    "    print(f\"train on {train_part}, test on {test_perplm}\")\n",
    "    display_test([test(test_df, fit)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "\n",
    "- $k$ is relatively stable to changes in partition, SNR; moreso than $a,b$\n",
    "    - $R^2$ is inflated by low SNRs by virtue of being near to the intercept\n",
    "- $k$ can probably be inferred from $a,b$\n",
    "    - $\\log b / a$ stabilizes as SNR increases. Lesser variance, maybe? How do we reconcile that $k$ is reasonably robust to changes in partition?\n",
    "        - Check if $\\log b / a$ converges to something else on `dev-other`. Perhaps it's close enough to the `dev-clean` ratio that drastic changes in entropy dominate?\n",
    "    - Based on its current trajectory, the ratio of $\\log b / a \\approx 12$ will never be exceeded by the entropy of the partition. The corresponding perplexity is in the vicinity of $162,000$.\n",
    "- $k$ can be estimated by a ratio of entropies\n",
    "    - As speech becomes cleaner, errors are more likely to occur one at a time. Guesswork more closely resembles the perplexity computations, which are conditioned on single words.\n",
    "- Serious problem with ratio estimates (Curran-Everett). $k$ may be compromised.\n",
    "    - Easy solution is to include intercepts. Regardless, $k$ can be used to predict with or without explaining.\n",
    "- Klakow's model predicts accuracy $b$ with $0$ entropy. However, $0$ entropy ought to be $0$ errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
